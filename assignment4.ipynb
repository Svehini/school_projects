{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a612e44c82956c2c8cb1fd0fac06b36",
     "grade": false,
     "grade_id": "cell-069100a95d5cf432",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Assignment 4\n",
    "\n",
    "Welcome to the last assignment! Here you will evaluate the recommenders you developed in the previous assignments. We will use part of the MovieLens 20M dataset.\n",
    "\n",
    "You will write and execute your code in Python using this Jupyter Notebook.\n",
    "\n",
    "**TASK:** Your job is to: (1) Copy your code from previous assignments into the correct place in the python files `Recommender_CB.py`, `Recommender_CF_UU.py`, and `Recommender_MF.py`.\n",
    "(2) Fill in the missing code in this notebook.\n",
    "In both cases, the place to enter your code is clearly marked with comments.\n",
    "\n",
    "**SUBMISSION:** You will submit this Notebook via the Interface of JupyterHub.\n",
    "\n",
    "- Submissions are possible until **25.04.2023 23:59 CEST**.\n",
    "- Do **NOT** rename the file it needs to be named as \"assignment4.ipynb\" (in the case if you want to run the Jupyter Notebook offline).\n",
    "- Please **save** (\"File -> \"Save and Checkpoint\") and **close** your Jupyter Notebook (\"file\" -> \"Close and Halt\") before you hand in your solution.\n",
    "- We will use our own python files to validate if your implementation in the `assignment4.ipynb` file is correct. You can submit your python files if you want or delete them before you submit. If you delete them you will get an error during validation (import failes obviously) in this case just ignore the error message.\n",
    "- Please note that the validation function only checks if there are syntax errors in your implementation. There is also a time-based constraint set to 30 seconds, which is too short in many cases. The resulting timeout error has nothing to do with the automatic evaluation and can be ignored.\n",
    "\n",
    "**PLEASE NOTE THE FOLLOWING:**\n",
    "- Fill only missing code. The place to enter your code is clearly marked with comments.\n",
    "- do not change the locked cells\n",
    "- check that there are no Python syntax errors\n",
    "- check your code and also if your code solves the required tasks\n",
    "- check if you have not created any infinite loops\n",
    "- A line/cell must not take longer than 10 min, otherwise the line will be marked as faulty during automatic evaluation\n",
    "\n",
    "**GRADING:** We will test whether your code produces the expected output. Therefore hidden tests will compare results of the standard solution with yours (based on the whole dataset - multiple, randomly selected inputs - accuracy of the solution must be within two decimal places). Note that the visible test cells are only an indicator for the correctness of your solution. They **do not** guarantee that your solution is correct. \n",
    "\n",
    "Late submissions are not possible. We will automatically collect all submissions at the end of the deadline!\n",
    "\n",
    "We reserve the right to carry out automatic plagiarism checks. Please do not exploit the submission system. We will look at all submissions and such submissions will be scored 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1ded2ae778e739a06d20be0d9d0358b",
     "grade": false,
     "grade_id": "cell-7d3996addbdb0513",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Preparation\n",
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b30083cc2482c2d09d331b064139cbd3",
     "grade": false,
     "grade_id": "cell-f551d844f4ff5a5b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#if you wish to disable warnings, uncomment the following two lines\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29ebd7ee7d785df9d2f268c4cd8eeb6a",
     "grade": false,
     "grade_id": "cell-be045e0c5e8f06b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=500, precision=4)\n",
    "pd.options.display.max_seq_items = 100\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "363622de52d6dfefaa635c1102fb5ca4",
     "grade": false,
     "grade_id": "cell-8ac40463e009e648",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to enter the correct location of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "data_directory = '~/shared/data/assignment3/'"
   },
   "outputs": [],
   "source": [
    "data_directory = '~/shared/data/assignment3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "027ab34b2e970f1f78e33122420f7b29",
     "grade": true,
     "grade_id": "load-data-for-grading",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33f90a4373ce7f732173fbfca75c0ebe",
     "grade": false,
     "grade_id": "cell-4f0e56c1ed0c9415",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Create the movies DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f136ecbb281f1e3e48137e3556a74db9",
     "grade": false,
     "grade_id": "cell-fc8433ac33942bd6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "links = pd.read_csv(data_directory + 'links.csv')\n",
    "movies_plain = pd.read_csv(data_directory + 'movies.csv')\n",
    "metadata = pd.read_csv(data_directory + 'movies_metadata.csv', low_memory=False)\n",
    "metadata.drop(metadata.columns[[0,1,2,4,6,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23]], axis=1, inplace=True)\n",
    "keywords = pd.read_csv(data_directory + 'keywords.csv', low_memory=False)\n",
    "credits = pd.read_csv(data_directory + 'credits.csv', low_memory=False)\n",
    "\n",
    "keywords['id'] = keywords['id'].astype('int')\n",
    "links=links[links['tmdbId'].isnull()==False]\n",
    "links['tmdbId'] = links['tmdbId'].astype('int')\n",
    "metadata = metadata.drop([19730, 29503, 35587])\n",
    "metadata['id'] = metadata['id'].astype('int')\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "\n",
    "movies = metadata.merge(links, how='inner', left_on='id', right_on='tmdbId')\n",
    "movies = movies.merge(movies_plain, how='inner', left_on='movieId', right_on='movieId')\n",
    "movies = movies.merge(keywords, how='inner', left_on='id', right_on='id')\n",
    "movies = movies.merge(credits, how='inner', left_on='id', right_on='id')\n",
    "movies = movies.drop(columns=['tmdbId','genres_y'])\n",
    "movies.rename(columns={'genres_x': 'genres'}, inplace=True)\n",
    "\n",
    "movies=movies[movies['overview'].isnull()==False]\n",
    "\n",
    "movies = movies[movies['movieId'] < 1000]\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    movies[feature] = movies[feature].apply(literal_eval)\n",
    "    \n",
    "\n",
    "# Get the director's name from the crew feature. If director is not listed, return NaN\n",
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan\n",
    "\n",
    "# Returns the list top 3 elements or entire list; whichever is more.\n",
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []\n",
    "\n",
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "movies['director'] = movies['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    movies[feature] = movies[feature].apply(get_list)\n",
    "\n",
    "    \n",
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        # Check if string exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    movies[feature] = movies[feature].apply(clean_data)\n",
    "\n",
    "    \n",
    "# Drop duplicate movies   \n",
    "import collections\n",
    "movie_ids = movies['movieId'].tolist()\n",
    "movie_ids_dup = [x for  x, y in collections.Counter(movie_ids).items() if y > 1]\n",
    "\n",
    "for movie_id in movie_ids_dup:\n",
    "    to_drop = movies.index[movies.movieId == movie_id].tolist()[1:]\n",
    "    movies.drop(to_drop, inplace=True)\n",
    "\n",
    "movies.drop(columns='crew', inplace=True)\n",
    "\n",
    "\n",
    "movies.rename(columns={'overview':'plot'}, inplace=True)\n",
    "\n",
    "def create_metadata(x):\n",
    "        return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])  \n",
    "\n",
    "# Create a new metadata feature\n",
    "movies['metadata'] = movies.apply(create_metadata, axis=1)\n",
    "\n",
    "display(movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "576580619281b281b5e36981a99ff7e2",
     "grade": false,
     "grade_id": "cell-6d741b436f10ed88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Create the ratings DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a45aec96b719de09660313163ef75e79",
     "grade": false,
     "grade_id": "cell-384094dd16b380a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(data_directory + 'ratings.csv')\n",
    "ratings = ratings.drop(columns=['timestamp'])\n",
    "ratings = ratings[(ratings['userId'] < 1000) & (ratings['movieId'] < 100) ]\n",
    "\n",
    "ratings = ratings[ratings['movieId'].isin(movies['movieId'])]\n",
    "\n",
    "## keep users with more than 2 ratings\n",
    "ratings_count = ratings.groupby(['userId', 'movieId']).size().groupby('userId').size()\n",
    "ratings_ok = ratings_count[ratings_count >= 2].reset_index()[['userId']]\n",
    "ratings = ratings.merge(ratings_ok, \n",
    "               how = 'right',\n",
    "               left_on = 'userId',\n",
    "               right_on = 'userId')\n",
    "\n",
    "\n",
    "ratings.columns = ['user', 'item', 'rating']\n",
    "\n",
    "display(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f33f08ecee6cb2910c9703e6a9452a5a",
     "grade": false,
     "grade_id": "cell-272d2ccda90715a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Split ratings into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea08575610f795f4917c4a0049c0e27b",
     "grade": false,
     "grade_id": "cell-663f8ce000d27624",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(ratings,\n",
    "                                               stratify=ratings['user'],\n",
    "                                               test_size=0.20,\n",
    "                                               random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "### keep only users which have at least one positive (>3) ratings in train\n",
    "positive_ratings = ratings_train[ratings_train['rating']>3]\n",
    "positive_userIds = positive_ratings['user'].unique()\n",
    "\n",
    "ratings_train = ratings_train[ratings_train['user'].isin(positive_userIds)]\n",
    "ratings_test = ratings_test[ratings_test['user'].isin(positive_userIds)]\n",
    "\n",
    "\n",
    "### keep in test only ratings for movies which appear in train\n",
    "ratings_test = ratings_test[ratings_test['item'].isin(ratings_train['item'])]\n",
    "\n",
    "item_ids = ratings_train['item'].unique()\n",
    "item_ids.sort()\n",
    "print(f'{len(item_ids)} items overall')\n",
    "\n",
    "print(len(ratings_train['user'].unique()), 'users,', len(ratings_train['item'].unique()), 'items,', len(ratings_train.index), 'ratings in train set')\n",
    "print(len(ratings_test['user'].unique()), 'users,', len(ratings_test['item'].unique()), 'items,', len(ratings_test.index), 'ratings in test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2e15158796166fb1e3455a4dd3a494d",
     "grade": false,
     "grade_id": "cell-c230756a3383f4c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## trim movies dataframe to contain only movies in item_ids\n",
    "\n",
    "movies[movies['movieId'].isin(item_ids)]\n",
    "\n",
    "movies.rename(columns={'movieId': 'item_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ea672aa8f10f51b77f955a8411afa75",
     "grade": false,
     "grade_id": "cell-138c02e4404292c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Import the recommenders --- TO EDIT EXTERNALLY\n",
    "\n",
    "You will implement the three recommenders from the previous assignments as classes in separate files. \n",
    "\n",
    "Fill in the missing code in the files `Recommender_CB.py`, `Recommender_CF_UU.py`, and `Recommender_MF.py`. For the biggest part, you have to copy over the code you wrote for the previous assignments. \n",
    "\n",
    "IMPORTANT: because the recommenders are now implemented as separate classes, you may have to prefix non local variables with `self.` so that they are visible.\n",
    "\n",
    "In general, the idea is to implement a simple recommender API:\n",
    "```\n",
    "class Recommender:  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_model(self, ratings_train, movies):\n",
    "        pass\n",
    "    \n",
    "    def recommend(self, user_id, item_ids=None, topN=20):\n",
    "        pass\n",
    "```\n",
    "\n",
    "Once done, we can import these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91cde06141585ade1e5a933977502d5f",
     "grade": false,
     "grade_id": "cell-a2893a35f4610135",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from Recommender_CB import Recommender_CB\n",
    "from Recommender_CF_UU import Recommender_CF_UU\n",
    "from Recommender_MF import Recommender_MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3eb1bb754f8c916df25d29c88f417c5a",
     "grade": true,
     "grade_id": "override_imports",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78b40cdcd2f48f4bdef5ce3803246daf",
     "grade": false,
     "grade_id": "cell-b72c3ccf15bf9cd0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Testing the recommenders\n",
    "\n",
    "Make sure you have correctly copied the code from previous assignments to the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67519e46ca5874db3a947ca24b8e4b3c",
     "grade": false,
     "grade_id": "cell-431fd1a980babe67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cbr = Recommender_CB('plot')\n",
    "cbr.build_model(ratings_train, movies)\n",
    "\n",
    "print(cbr.recommend(10))\n",
    "print(cbr.recommend(100))\n",
    "print(cbr.recommend(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca938d8f5fe667d167f7c19caefe922",
     "grade": false,
     "grade_id": "cell-7b070ab7517ecb80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "[16, 36, 21, 8, 94, 69, 84, 76, 18, 60, 52, 35, 89, 57, 23, 58, 34, 55, 83, 2]\n",
    "[16, 36, 8, 21, 94, 69, 76, 35, 89, 52, 23, 84, 99, X, X, X, X, X, X, X]\n",
    "[92, 71, 50, 37, 65, 29, 61, 79, 39, 47, 2, 84, 88, 73, 21, 86, 51, 7, 13, 67]\n",
    "```\n",
    "where `X` means any item id, as from this point on all remaining items have zero similarity with the user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "facb1f7ca19391d5c4442d246f83ff7e",
     "grade": false,
     "grade_id": "cell-b7a9dfaa46811e80",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "uucf = Recommender_CF_UU()\n",
    "uucf.build_model(ratings_train)\n",
    "\n",
    "print(uucf.recommend(10))\n",
    "print(uucf.recommend(100))\n",
    "print(uucf.recommend(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb08d5589978f40a60a212c9b0483228",
     "grade": false,
     "grade_id": "cell-2fa5a36ea3cfbb98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "[38, 65, 35, 67, 73, 12, 37, 63, 82, 54, 50, 66, 60, 34, 47, 17, 18, 97, 4, 45]\n",
    "[67, 90, 97, 50, 73, 94, 16, 47, 58, 55, 26, 78, 99, 82, 57, 85, 61, 11, 25, 84]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9940eff25fc28891f6fe6d036dee350a",
     "grade": false,
     "grade_id": "cell-6dd449713e92bec2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "mfr = Recommender_MF()\n",
    "mfr.build_model(ratings_train)\n",
    "\n",
    "print(mfr.recommend(10))\n",
    "print(mfr.recommend(100))\n",
    "print(mfr.recommend(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceb20107af0cd245fb69d4b63367ff7c",
     "grade": false,
     "grade_id": "cell-7974847f26dc0174",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "[50, 47, 17, 6, 32, 36, 16, 62, 58, 73, 29, 72, 11, 82, 84, 41, 96, 8, 7, 92]\n",
    "[50, 47, 1, 17, 6, 36, 16, 58, 62, 73, 72, 11, 29, 41, 21, 92, 82, 84, 42, 74]\n",
    "[50, 47, 1, 6, 25, 36, 58, 16, 72, 29, 62, 11, 73, 82, 42, 92, 84, 34, 21, 41]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edf401462ce9b01dd0872995f0033278",
     "grade": false,
     "grade_id": "cell-5c66b0645f918219",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "## Implement the Evaluator --- TO EDIT\n",
    "\n",
    "The following class evaluates the ranking produced by a recommender.\n",
    "\n",
    "You need to edit the following functions:\n",
    "- `get_ground_truth` in 1 place\n",
    "- `get_ranking_metrics` in 3 places\n",
    "\n",
    "\n",
    "**NOTE:** You should implement the following variant of DCG (there is a slight difference from the course slides):\n",
    "\n",
    "$$\\text{DCG}@k = \\sum_{i=1}^k \\frac{2^{rel_i}-1}{\\log(i+1)}$$\n",
    "\n",
    "where $rel_i$ is the relevance score of the item at position $i$ in the ranking. The definition of IDCG, and thus NDCG, is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d8f7ba1ad04d265e8de2b18db565e4c",
     "grade": false,
     "grade_id": "Evaluator",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "revert": "import math\nimport random\nimport warnings\n\n\nDEBUG = True\n\nif DEBUG:\n    random.seed(42)\n\nclass Evaluator:\n    \n    def __init__(self, topN=20):\n        self.topN = topN\n\n    \n    def init_data(self, ratings_train, ratings_test):\n        self.ratings_train = ratings_train\n        self.ratings_test = ratings_test\n        self.find_unrated_items()\n    \n    \n    ### store for each user her/his unrated items\n    def find_unrated_items(self):\n        all_items = set(self.ratings_train['item'].tolist())\n        \n        self.unrated = {}\n        \n        for user_id in self.ratings_train['user'].unique():\n            rated_train_items = self.ratings_train[self.ratings_train['user'] == user_id]['item'].tolist()\n            rated_test_items = self.ratings_test[self.ratings_test['user'] == user_id]['item'].tolist()\n\n            rated_items = set(rated_train_items) | set(rated_test_items) # union of sets\n            unrated_items = list(all_items - rated_items)\n            random.shuffle(unrated_items)\n            \n            self.unrated[user_id] = unrated_items\n            \n    \n    ### get the ratings of user_id in ratings_test\n    def get_ground_truth(self, user_id):\n        \n        ## get the test ratings of user_id as a DataFrame subset of `self.ratings_test`\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n        ## dictionary of ground truth ratings\n        ground_truth = pd.Series(user_ratings['rating'].values, index=user_ratings['item']).to_dict()\n\n        return ground_truth\n    \n    \n    def get_recommendations(self, model, user_id):\n        ground_truth = self.get_ground_truth(user_id)\n        n_test = len(ground_truth)\n        \n        ## we will create a total of topN items, and ask the recommender to rank them\n        ## among these items, we will include the ground truth items\n        \n        ## 1. select (topN - n_test) unrated items\n        item_ids = self.unrated[user_id][:self.topN - n_test]\n        \n        ## 2; add ground truth items\n        item_ids = item_ids + list(ground_truth.keys())\n        \n        ## get the model's ranking\n        recommendations = model.recommend(user_id, item_ids, self.topN)\n        return recommendations\n    \n    \n    ### evaluate the model on given user_id\n    def eval_model_on_user(self, model, user_id, verbose=True):\n        ground_truth = self.get_ground_truth(user_id)\n        if verbose:\n            print('ground truth', ground_truth)\n        n_test = len(ground_truth)\n        \n        ## we will create a total of topN items, and ask the recommender to rank them\n        ## among these items, we will include the ground truth items\n        \n        ## 1. select (topN - n_test) unrated items\n        item_ids = self.unrated[user_id][:self.topN - n_test]\n        \n        ## 2; add ground truth items\n        item_ids = item_ids + list(ground_truth.keys())\n        \n        ## get the model's ranking\n        recommendations = model.recommend(user_id, item_ids, self.topN)\n        if verbose:\n            print('recommendations', recommendations)\n        \n        ## evaluate the ranking\n        metrics = self.get_ranking_metrics(ground_truth, recommendations)\n        \n        return metrics\n    \n    \n    ### evaluate the model on all users\n    def eval_model(self, model, n_users=-1):\n        metrics_all = []\n        count = 0;\n        for user_id in self.ratings_train['user'].unique():\n            count+=1\n            print(\"\\r\", \"evaluated on \", count, \" users\", end=\"\", sep=\"\")\n            metrics = self.eval_model_on_user(model, user_id, verbose=False)\n            if metrics is None:\n                continue\n            metrics_all.append(metrics)\n            if count == n_users:\n                break\n        \n        print(\"\\n\")\n        \n        \n        ## store all metrics in a DataFrame for easy manipulation\n        metrics_all_df = pd.DataFrame(metrics_all)\n        self.metrics_all_df = metrics_all_df        \n        \n        ## average over all metrics\n        hits_array = metrics_all_df.hits\n        hits = np.nanmean(hits_array)\n        ap_array = metrics_all_df.ap\n        ap = np.nanmean(ap_array)\n        \n        rec_array = np.vstack(metrics_all_df.rec)\n        prec_array = np.vstack(metrics_all_df.prec)\n        ndcg_array = np.vstack(metrics_all_df.ndcg)\n        \n        \n        with warnings.catch_warnings(): ## ignore division by 0\n            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n            rec = np.nanmean(rec_array, axis=0)\n            prec = np.nanmean(prec_array, axis=0)\n            ndcg = np.nanmean(ndcg_array, axis=0)\n        \n        \n        metrics_avg = {'hits':hits,\n                   'ap':ap,\n                   'rec':np.array(rec),\n                   'prec':np.array(prec),\n                   'ndcg':np.array(ndcg)}\n        \n        return metrics_avg\n        \n    ### get some evaluation metrics for ranking with respect to ground_truth\n    def get_ranking_metrics(self, ground_truth, ranking):\n        n_test = len(ground_truth)\n        if n_test == 0:\n            return None\n        \n        hits = 0 ## number of relevant in ranking\n        rec = [] ## recall at every position of ranking\n        prec = [] ## precision at every position of ranking\n        dcg = [] ## DCG at every position of ranking\n        ap = 0 ## average precision\n        \n                \n        ## scan the ranking and compute hits, rec, prec, dcg, ap\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n        if (hits != 0):\n            ap /= hits\n        else:\n            ap = 0\n        \n        ## constuct the ideal ranking from ground truth to compute idcg\n        ideal = sorted(ground_truth, key=ground_truth.get, reverse=True)\n        idcg = []\n        \n        ## scan the ideal ranking and compute idcg\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n        \n        ## make sure the dcg and idcg lists have the same length\n        if len(ideal) >= len(ranking):\n            idcg = idcg[:len(ranking)]\n        else:\n            last_idcg = idcg[-1]\n            for i in range(len(ranking) - len(ideal)):\n                idcg.append(last_idcg)\n        \n        ## compute NDCG = DCG/IDCG\n        ## TIP convert lists to `np.array` to do the division and then back to a list with `.tolist()`\n        # YOUR CODE HERE\n        raise NotImplementedError()\n        \n        rec = np.array(rec)\n        prec = np.array(prec)\n        ndcg = np.array(ndcg)\n        \n        ## make them have length self.topN, fill in with nan \n        rec = np.append(rec, np.repeat(np.nan, self.topN - len(rec)))\n        prec = np.append(prec, np.repeat(np.nan, self.topN - len(prec)))\n        ndcg = np.append(ndcg, np.repeat(np.nan, self.topN - len(ndcg)))\n        \n        metrics = {'hits':hits,\n                   'ap':ap,\n                   'rec':np.array(rec),\n                   'prec':np.array(prec),\n                   'ndcg':np.array(ndcg)}\n        \n        return metrics"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "    random.seed(42)\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, topN=20):\n",
    "        self.topN = topN\n",
    "\n",
    "    \n",
    "    def init_data(self, ratings_train, ratings_test):\n",
    "        self.ratings_train = ratings_train\n",
    "        self.ratings_test = ratings_test\n",
    "        self.find_unrated_items()\n",
    "    \n",
    "    \n",
    "    ### store for each user her/his unrated items\n",
    "    def find_unrated_items(self):\n",
    "        all_items = set(self.ratings_train['item'].tolist())\n",
    "        \n",
    "        self.unrated = {}\n",
    "        \n",
    "        for user_id in self.ratings_train['user'].unique():\n",
    "            rated_train_items = self.ratings_train[self.ratings_train['user'] == user_id]['item'].tolist()\n",
    "            rated_test_items = self.ratings_test[self.ratings_test['user'] == user_id]['item'].tolist()\n",
    "\n",
    "            rated_items = set(rated_train_items) | set(rated_test_items) # union of sets\n",
    "            unrated_items = list(all_items - rated_items)\n",
    "            random.shuffle(unrated_items)\n",
    "            \n",
    "            self.unrated[user_id] = unrated_items\n",
    "            \n",
    "    \n",
    "    ### get the ratings of user_id in ratings_test\n",
    "    def get_ground_truth(self, user_id):\n",
    "        \n",
    "        ## get the test ratings of user_id as a DataFrame subset of `self.ratings_test`\n",
    "        # YOUR CODE HERE\n",
    "        user_ratings = self.ratings_test.loc[self.ratings_test[\"user\"]==user_id]\n",
    "        \n",
    "        ## dictionary of ground truth ratings\n",
    "        ground_truth = pd.Series(user_ratings['rating'].values, index=user_ratings['item']).to_dict()\n",
    "\n",
    "        return ground_truth\n",
    "    \n",
    "    \n",
    "    def get_recommendations(self, model, user_id):\n",
    "        ground_truth = self.get_ground_truth(user_id)\n",
    "        n_test = len(ground_truth)\n",
    "        \n",
    "        ## we will create a total of topN items, and ask the recommender to rank them\n",
    "        ## among these items, we will include the ground truth items\n",
    "        \n",
    "        ## 1. select (topN - n_test) unrated items\n",
    "        item_ids = self.unrated[user_id][:self.topN - n_test]\n",
    "        \n",
    "        ## 2; add ground truth items\n",
    "        item_ids = item_ids + list(ground_truth.keys())\n",
    "        \n",
    "        ## get the model's ranking\n",
    "        recommendations = model.recommend(user_id, item_ids, self.topN)\n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    ### evaluate the model on given user_id\n",
    "    def eval_model_on_user(self, model, user_id, verbose=True):\n",
    "        ground_truth = self.get_ground_truth(user_id)\n",
    "        if verbose:\n",
    "            print('ground truth', ground_truth)\n",
    "        n_test = len(ground_truth)\n",
    "        \n",
    "        ## we will create a total of topN items, and ask the recommender to rank them\n",
    "        ## among these items, we will include the ground truth items\n",
    "        \n",
    "        ## 1. select (topN - n_test) unrated items\n",
    "        item_ids = self.unrated[user_id][:self.topN - n_test]\n",
    "        \n",
    "        ## 2; add ground truth items\n",
    "        item_ids = item_ids + list(ground_truth.keys())\n",
    "        \n",
    "        ## get the model's ranking\n",
    "        recommendations = model.recommend(user_id, item_ids, self.topN)\n",
    "        if verbose:\n",
    "            print('recommendations', recommendations)\n",
    "        \n",
    "        ## evaluate the ranking\n",
    "        metrics = self.get_ranking_metrics(ground_truth, recommendations)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    ### evaluate the model on all users\n",
    "    def eval_model(self, model, n_users=-1):\n",
    "        metrics_all = []\n",
    "        count = 0;\n",
    "        for user_id in self.ratings_train['user'].unique():\n",
    "            count+=1\n",
    "            print(\"\\r\", \"evaluated on \", count, \" users\", end=\"\", sep=\"\")\n",
    "            metrics = self.eval_model_on_user(model, user_id, verbose=False)\n",
    "            if metrics is None:\n",
    "                continue\n",
    "            metrics_all.append(metrics)\n",
    "            if count == n_users:\n",
    "                break\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        ## store all metrics in a DataFrame for easy manipulation\n",
    "        metrics_all_df = pd.DataFrame(metrics_all)\n",
    "        self.metrics_all_df = metrics_all_df        \n",
    "        \n",
    "        ## average over all metrics\n",
    "        hits_array = metrics_all_df.hits\n",
    "        hits = np.nanmean(hits_array)\n",
    "        ap_array = metrics_all_df.ap\n",
    "        ap = np.nanmean(ap_array)\n",
    "        \n",
    "        rec_array = np.vstack(metrics_all_df.rec)\n",
    "        prec_array = np.vstack(metrics_all_df.prec)\n",
    "        ndcg_array = np.vstack(metrics_all_df.ndcg)\n",
    "        \n",
    "        \n",
    "        with warnings.catch_warnings(): ## ignore division by 0\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            rec = np.nanmean(rec_array, axis=0)\n",
    "            prec = np.nanmean(prec_array, axis=0)\n",
    "            ndcg = np.nanmean(ndcg_array, axis=0)\n",
    "        \n",
    "        \n",
    "        metrics_avg = {'hits':hits,\n",
    "                   'ap':ap,\n",
    "                   'rec':np.array(rec),\n",
    "                   'prec':np.array(prec),\n",
    "                   'ndcg':np.array(ndcg)}\n",
    "        \n",
    "        return metrics_avg\n",
    "        \n",
    "    ### get some evaluation metrics for ranking with respect to ground_truth\n",
    "    def get_ranking_metrics(self, ground_truth, ranking):\n",
    "        n_test = len(ground_truth)\n",
    "        if n_test == 0:\n",
    "            return None\n",
    "        \n",
    "        hits = 0 ## number of relevant in ranking\n",
    "        rec = [] ## recall at every position of ranking\n",
    "        prec = [] ## precision at every position of ranking\n",
    "        dcg = [] ## DCG at every position of ranking\n",
    "        ap = 0 ## average precision\n",
    "        \n",
    "                \n",
    "        ## scan the ranking and compute hits, rec, prec, dcg, ap\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        ground_truth = dict(ground_truth)\n",
    "        for key, value in ground_truth.items():\n",
    "            if key in ranking:\n",
    "                if value>0:\n",
    "                    hits+=1\n",
    "        \n",
    "        prec_num = 0; rec_num = 0; dcg = []; dcg_val = 0\n",
    "        for i in range(len(ranking)):\n",
    "            if ranking[i] in ground_truth.keys():\n",
    "                value = ground_truth[ranking[i]]\n",
    "                if value>0:\n",
    "                    prec_num+=1\n",
    "                    rec_num+=(1/hits)\n",
    "                    ap+=round(prec_num/(i+1), 3)\n",
    "                dcg_val += ((2**(value))-1)/math.log((i+1)+1)\n",
    "            else:\n",
    "                dcg_val += 0\n",
    "            dcg.append(dcg_val)\n",
    "            prec.append(round(prec_num/(i+1), 3))\n",
    "            rec.append(round(rec_num, 3))\n",
    "        \n",
    "        if (hits != 0):\n",
    "            ap /= hits\n",
    "        else:\n",
    "            ap = 0\n",
    "        \n",
    "        ## constuct the ideal ranking from ground truth to compute idcg\n",
    "        ideal = sorted(ground_truth, key=ground_truth.get, reverse=True)\n",
    "        idcg = []\n",
    "        \n",
    "        ## scan the ideal ranking and compute idcg\n",
    "        # YOUR CODE HERE\n",
    "        idcg_val = 0\n",
    "        for i in range(len(ideal)):\n",
    "            if ideal[i] in ground_truth.keys():\n",
    "                value = ground_truth[ideal[i]]\n",
    "                idcg_val += ((2**(value))-1)/math.log((i+1)+1)\n",
    "            else:\n",
    "                idcg_val += 0\n",
    "            idcg.append(idcg_val)\n",
    "        \n",
    "        \n",
    "        ## make sure the dcg and idcg lists have the same length\n",
    "        if len(ideal) >= len(ranking):\n",
    "            idcg = idcg[:len(ranking)]\n",
    "        else:\n",
    "            last_idcg = idcg[-1]\n",
    "            for i in range(len(ranking) - len(ideal)):\n",
    "                idcg.append(last_idcg)\n",
    "        \n",
    "        ## compute NDCG = DCG/IDCG\n",
    "        ## TIP convert lists to `np.array` to do the division and then back to a list with `.tolist()`\n",
    "        # YOUR CODE HERE\n",
    "        ndcg = []\n",
    "        for i in range(len(dcg)):\n",
    "            ndcg.append(dcg[i]/idcg[i])\n",
    "        \n",
    "        rec = np.array(rec)\n",
    "        prec = np.array(prec)\n",
    "        ndcg = np.array(ndcg)\n",
    "        \n",
    "        ## make them have length self.topN, fill in with nan \n",
    "        rec = np.append(rec, np.repeat(np.nan, self.topN - len(rec)))\n",
    "        prec = np.append(prec, np.repeat(np.nan, self.topN - len(prec)))\n",
    "        ndcg = np.append(ndcg, np.repeat(np.nan, self.topN - len(ndcg)))\n",
    "        \n",
    "        metrics = {'hits':hits,\n",
    "                   'ap':ap,\n",
    "                   'rec':np.array(rec),\n",
    "                   'prec':np.array(prec),\n",
    "                   'ndcg':np.array(ndcg)}\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0aa4644491aa40fbc9e2196838a3ec2b",
     "grade": false,
     "grade_id": "cell-1c036415aeba961a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Test the ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18e71624301025179720bfe15b31094e",
     "grade": false,
     "grade_id": "cell-febb067df9f094e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "evl = Evaluator(topN = 10)\n",
    "\n",
    "ground_truth = {200:5, 100:4, 400:3, 1000:3}\n",
    "ranking = list(range(100, 1100, 100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:4, 200:3, 400:3, 1000:3}\n",
    "ranking = list(range(100, 1100, 100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:1, 200:1, 300:1, 400:1}\n",
    "ranking = list(range(100, 1100, 100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:1, 200:1, 300:1, 400:1}\n",
    "ranking = list(range(100, 1100, 100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {200:5, 100:4, 400:3, 1000:3}\n",
    "ranking = list(range(1000, 0, -100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:4, 200:3, 400:3, 1000:3}\n",
    "ranking = list(range(1000, 0, -100))\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "\n",
    "\n",
    "evl = Evaluator(topN = 5)\n",
    "\n",
    "ground_truth = {200:5, 100:4, 400:3, 1000:3}\n",
    "ranking = list(range(100, 1100, 100))[:5]\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:4, 200:3, 400:3, 1000:3}\n",
    "ranking = list(range(100, 1100, 100))[:5]\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:1, 200:1, 300:1, 400:1}\n",
    "ranking = list(range(100, 1100, 100))[:5]\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))\n",
    "\n",
    "ground_truth = {100:1, 200:1, 300:1, 400:1}\n",
    "ranking = list(range(100, 1100, 100))[:5]\n",
    "display(evl.get_ranking_metrics(ground_truth, ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c5366495df5360c143ac8c98693ec04",
     "grade": false,
     "grade_id": "cell-77f3977356f7ebe0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "{'hits': 4,\n",
    " 'ap': 0.7875,\n",
    " 'rec': array([0.25, 0.5 , 0.5 , 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1.  ]),\n",
    " 'prec': array([1.    , 1.    , 0.6667, 0.75  , 0.6   , 0.5   , 0.4286, 0.375 ,\n",
    "        0.3333, 0.4   ]),\n",
    " 'ndcg': array([0.4839, 0.8541, 0.7861, 0.7998, 0.7998, 0.7998, 0.7998, 0.7998,\n",
    "        0.7998, 0.8429])}\n",
    "{'hits': 4,\n",
    " 'ap': 0.7875,\n",
    " 'rec': array([0.25, 0.5 , 0.5 , 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 1.  ]),\n",
    " 'prec': array([1.    , 1.    , 0.6667, 0.75  , 0.6   , 0.5   , 0.4286, 0.375 ,\n",
    "        0.3333, 0.4   ]),\n",
    " 'ndcg': array([1.    , 1.    , 0.8473, 0.865 , 0.865 , 0.865 , 0.865 , 0.865 ,\n",
    "        0.865 , 0.9431])}\n",
    "{'hits': 4,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([0.25, 0.5 , 0.75, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ]),\n",
    " 'prec': array([1.    , 1.    , 1.    , 1.    , 0.8   , 0.6667, 0.5714, 0.5   ,\n",
    "        0.4444, 0.4   ]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
    "{'hits': 4,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([0.25, 0.5 , 0.75, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ]),\n",
    " 'prec': array([1.    , 1.    , 1.    , 1.    , 0.8   , 0.6667, 0.5714, 0.5   ,\n",
    "        0.4444, 0.4   ]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
    "{'hits': 4,\n",
    " 'ap': 0.5048,\n",
    " 'rec': array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5 , 0.5 , 0.75, 1.  ]),\n",
    " 'prec': array([1.    , 0.5   , 0.3333, 0.25  , 0.2   , 0.1667, 0.2857, 0.25  ,\n",
    "        0.3333, 0.4   ]),\n",
    " 'ndcg': array([0.2258, 0.173 , 0.1592, 0.149 , 0.149 , 0.149 , 0.1987, 0.1987,\n",
    "        0.3973, 0.4896])}\n",
    "{'hits': 4,\n",
    " 'ap': 0.5048,\n",
    " 'rec': array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5 , 0.5 , 0.75, 1.  ]),\n",
    " 'prec': array([1.    , 0.5   , 0.3333, 0.25  , 0.2   , 0.1667, 0.2857, 0.25  ,\n",
    "        0.3333, 0.4   ]),\n",
    " 'ndcg': array([0.4667, 0.3605, 0.3055, 0.2699, 0.2699, 0.2699, 0.3599, 0.3599,\n",
    "        0.4412, 0.6084])}\n",
    "{'hits': 3,\n",
    " 'ap': 0.9167,\n",
    " 'rec': array([0.25, 0.5 , 0.5 , 0.75, 0.75]),\n",
    " 'prec': array([1.    , 1.    , 0.6667, 0.75  , 0.6   ]),\n",
    " 'ndcg': array([0.4839, 0.8541, 0.7861, 0.7998, 0.7998])}\n",
    "{'hits': 3,\n",
    " 'ap': 0.9167,\n",
    " 'rec': array([0.25, 0.5 , 0.5 , 0.75, 0.75]),\n",
    " 'prec': array([1.    , 1.    , 0.6667, 0.75  , 0.6   ]),\n",
    " 'ndcg': array([1.    , 1.    , 0.8473, 0.865 , 0.865 ])}\n",
    "{'hits': 4,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([0.25, 0.5 , 0.75, 1.  , 1.  ]),\n",
    " 'prec': array([1. , 1. , 1. , 1. , 0.8]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1.])}\n",
    "{'hits': 4,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([0.25, 0.5 , 0.75, 1.  , 1.  ]),\n",
    " 'prec': array([1. , 1. , 1. , 1. , 0.8]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1.])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbcd1d59bae748a6dd6e2d80a22e1b19",
     "grade": false,
     "grade_id": "cell-d5effef25a27e666",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Test the Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16a0d696167e6d2aad7e6c6d8ca8325f",
     "grade": false,
     "grade_id": "cell-996c4b02b2f83d81",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "evl = Evaluator(topN = 20)\n",
    "\n",
    "evl.init_data(ratings_train, ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86c10d5f3eb107b4a0565fde97f0c10f",
     "grade": false,
     "grade_id": "cell-cbb2c915a37988de",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Show the ground truth for users 10, 100, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "205a7b3dd3964364ca2dded7c479edcc",
     "grade": false,
     "grade_id": "cell-fbf0a67f0f3df975",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "display(evl.get_ground_truth(10))\n",
    "display(evl.get_ground_truth(100))\n",
    "display(evl.get_ground_truth(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af1b0d0a7e0652dcada7aa42554004e",
     "grade": false,
     "grade_id": "cell-0a4b4fe352d5c175",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate UU-CF on users 10, 100, 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e4e314d47f7090c55dec657e6ccae90",
     "grade": false,
     "grade_id": "cell-258cec54a0c644f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "display(evl.eval_model_on_user(uucf, 10))\n",
    "display(evl.eval_model_on_user(uucf, 100))\n",
    "display(evl.eval_model_on_user(uucf, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f03aa4b83f353e673832b78b87d5da87",
     "grade": false,
     "grade_id": "cell-5717f332fbd09a4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "ground truth {11: 4.0}\n",
    "recommendations [83, 37, 16, 38, 34, 14, 26, 78, 3, 86, 77, 31, 67, 79, 4, 49, 68, 94, 20, 11]\n",
    "{'hits': 1,\n",
    " 'ap': 0.0500,\n",
    " 'rec': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 1.]),\n",
    " 'prec': array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
    "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05]),\n",
    " 'ndcg': array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.    , 0.    , 0.2277])}\n",
    "ground truth {50: 5.0}\n",
    "recommendations [38, 65, 67, 50, 28, 6, 96, 94, 10, 95, 24, 7, 31, 89, 77, 71, 46, 93, 5, 51]\n",
    "{'hits': 1,\n",
    " 'ap': 0.2500,\n",
    " 'rec': array([0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.]),\n",
    " 'prec': array([0.    , 0.    , 0.    , 0.25  , 0.2   , 0.1667, 0.1429, 0.125 ,\n",
    "        0.1111, 0.1   , 0.0909, 0.0833, 0.0769, 0.0714, 0.0667, 0.0625,\n",
    "        0.0588, 0.0556, 0.0526, 0.05  ]),\n",
    " 'ndcg': array([0.    , 0.    , 0.    , 0.4307, 0.4307, 0.4307, 0.4307, 0.4307,\n",
    "        0.4307, 0.4307, 0.4307, 0.4307, 0.4307, 0.4307, 0.4307, 0.4307,\n",
    "        0.4307, 0.4307, 0.4307, 0.4307])}\n",
    "ground truth {6: 5.0, 21: 5.0}\n",
    "recommendations [94, 6, 41, 69, 4, 3, 76, 21, 40, 8, 24, 39, 88, 23, 15, 37, 49, 71, 56, 51]\n",
    "{'hits': 2,\n",
    " 'ap': 0.3750,\n",
    " 'rec': array([0. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1. , 1. , 1. , 1. , 1. , 1. ,\n",
    "        1. , 1. , 1. , 1. , 1. , 1. , 1. ]),\n",
    " 'prec': array([0.    , 0.5   , 0.3333, 0.25  , 0.2   , 0.1667, 0.1429, 0.25  ,\n",
    "        0.2222, 0.2   , 0.1818, 0.1667, 0.1538, 0.1429, 0.1333, 0.125 ,\n",
    "        0.1176, 0.1111, 0.1053, 0.1   ]),\n",
    " 'ndcg': array([0.    , 0.3869, 0.3869, 0.3869, 0.3869, 0.3869, 0.3869, 0.5803,\n",
    "        0.5803, 0.5803, 0.5803, 0.5803, 0.5803, 0.5803, 0.5803, 0.5803,\n",
    "        0.5803, 0.5803, 0.5803, 0.5803])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8bd267abd69dc6fde5f0b9787669482",
     "grade": false,
     "grade_id": "cell-23adf76f9eee8fda",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate MF on users 10, 100, 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfe1e1a99afe9e18ce110b3d12a3b24d",
     "grade": false,
     "grade_id": "cell-1318c7e03a097588",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "display(evl.eval_model_on_user(mfr, 10))\n",
    "display(evl.eval_model_on_user(mfr, 100))\n",
    "display(evl.eval_model_on_user(mfr, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dff5abc6ebf882a00c72d6af10ff8e7",
     "grade": false,
     "grade_id": "cell-96840db2a95297af",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "ground truth {11: 4.0}\n",
    "recommendations [16, 11, 67, 37, 34, 14, 86, 79, 26, 94, 77, 78, 68, 31, 38, 49, 83, 20, 4, 3]\n",
    "{'hits': 1,\n",
    " 'ap': 0.5000,\n",
    " 'rec': array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.]),\n",
    " 'prec': array([0.    , 0.5   , 0.3333, 0.25  , 0.2   , 0.1667, 0.1429, 0.125 ,\n",
    "        0.1111, 0.1   , 0.0909, 0.0833, 0.0769, 0.0714, 0.0667, 0.0625,\n",
    "        0.0588, 0.0556, 0.0526, 0.05  ]),\n",
    " 'ndcg': array([0.    , 0.6309, 0.6309, 0.6309, 0.6309, 0.6309, 0.6309, 0.6309,\n",
    "        0.6309, 0.6309, 0.6309, 0.6309, 0.6309, 0.6309, 0.6309, 0.6309,\n",
    "        0.6309, 0.6309, 0.6309, 0.6309])}\n",
    "ground truth {50: 5.0}\n",
    "recommendations [50, 6, 7, 67, 96, 28, 89, 77, 71, 51, 94, 10, 46, 38, 31, 5, 24, 95, 65, 93]\n",
    "{'hits': 1,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.]),\n",
    " 'prec': array([1.    , 0.5   , 0.3333, 0.25  , 0.2   , 0.1667, 0.1429, 0.125 ,\n",
    "        0.1111, 0.1   , 0.0909, 0.0833, 0.0769, 0.0714, 0.0667, 0.0625,\n",
    "        0.0588, 0.0556, 0.0526, 0.05  ]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.])}\n",
    "ground truth {6: 5.0, 21: 5.0}\n",
    "recommendations [6, 21, 41, 8, 56, 37, 76, 88, 51, 40, 71, 69, 94, 23, 4, 49, 15, 24, 3, 39]\n",
    "{'hits': 2,\n",
    " 'ap': 1.0000,\n",
    " 'rec': array([0.5, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
    "        1. , 1. , 1. , 1. , 1. , 1. , 1. ]),\n",
    " 'prec': array([1.    , 1.    , 0.6667, 0.5   , 0.4   , 0.3333, 0.2857, 0.25  ,\n",
    "        0.2222, 0.2   , 0.1818, 0.1667, 0.1538, 0.1429, 0.1333, 0.125 ,\n",
    "        0.1176, 0.1111, 0.1053, 0.1   ]),\n",
    " 'ndcg': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aabb664a7a29d66191c2e5f665f6a5a",
     "grade": false,
     "grade_id": "cell-7c1fda8531adb931",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate CB on users 10, 100, 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13f60171ab0fb7e9adcc219421a6e066",
     "grade": false,
     "grade_id": "cell-675524a6779336e1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "display(evl.eval_model_on_user(cbr, 10))\n",
    "display(evl.eval_model_on_user(cbr, 100))\n",
    "display(evl.eval_model_on_user(cbr, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fea7cee56c7b82db63bc22895faeb4ba",
     "grade": false,
     "grade_id": "cell-7716724e3b1a31fb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "ground truth {11: 4.0}\n",
    "recommendations [16, 94, 34, 83, 77, 49, 37, 31, 38, 11, 14, 26, 3, 4, 20, 78, 79, 67, 68, 86]\n",
    "{'hits': 1,\n",
    " 'ap': 0.1000,\n",
    " 'rec': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1.]),\n",
    " 'prec': array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.1   , 0.0909, 0.0833, 0.0769, 0.0714, 0.0667, 0.0625,\n",
    "        0.0588, 0.0556, 0.0526, 0.05  ]),\n",
    " 'ndcg': array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.2891, 0.2891, 0.2891, 0.2891, 0.2891, 0.2891, 0.2891,\n",
    "        0.2891, 0.2891, 0.2891, 0.2891])}\n",
    "ground truth {50: 5.0}\n",
    "recommendations [94, 89, 96, 95, 93, 77, 71, 67, 65, 31, 28, 24, 10, 7, 6, 5, 51, 50, 46, 38]\n",
    "{'hits': 1,\n",
    " 'ap': 0.0556,\n",
    " 'rec': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        1., 1., 1.]),\n",
    " 'prec': array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.0556, 0.0526, 0.05  ]),\n",
    " 'ndcg': array([0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
    "        0.    , 0.2354, 0.2354, 0.2354])}\n",
    "ground truth {6: 5.0, 21: 5.0}\n",
    "recommendations [71, 37, 39, 88, 21, 51, 23, 40, 94, 8, 24, 76, 3, 41, 49, 4, 6, 15, 56, 69]\n",
    "{'hits': 2,\n",
    " 'ap': 0.1588,\n",
    " 'rec': array([0. , 0. , 0. , 0. , 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
    "        0.5, 0.5, 0.5, 1. , 1. , 1. , 1. ]),\n",
    " 'prec': array([0.    , 0.    , 0.    , 0.    , 0.2   , 0.1667, 0.1429, 0.125 ,\n",
    "        0.1111, 0.1   , 0.0909, 0.0833, 0.0769, 0.0714, 0.0667, 0.0625,\n",
    "        0.1176, 0.1111, 0.1053, 0.1   ]),\n",
    " 'ndcg': array([0.    , 0.    , 0.    , 0.    , 0.2372, 0.2372, 0.2372, 0.2372,\n",
    "        0.2372, 0.2372, 0.2372, 0.2372, 0.2372, 0.2372, 0.2372, 0.2372,\n",
    "        0.3842, 0.3842, 0.3842, 0.3842])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97648a8c00aa991bffee16bdf1272dd2",
     "grade": false,
     "grade_id": "cell-606764c07c2d13ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate UU-CF on all users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04e47edb460f968fe7abec724f27ebc6",
     "grade": false,
     "grade_id": "cell-e6b0b20d09c7b3fe",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%time metrics = evl.eval_model(uucf)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d792d767deec36c19dda429276b8976b",
     "grade": false,
     "grade_id": "cell-601d6870b0694494",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "evaluated on 665 users\n",
    "\n",
    "CPU times: user 16 s, sys: 716 ms, total: 16.7 s\n",
    "Wall time: 16.7 s\n",
    "{'hits': 1.9125874125874125,\n",
    " 'ap': 0.22099337649325082,\n",
    " 'rec': array([0.0261, 0.0876, 0.1472, 0.215 , 0.2773, 0.3395, 0.3999, 0.4783,\n",
    "        0.5316, 0.5972, 0.6444, 0.6899, 0.7376, 0.7845, 0.8239, 0.8722,\n",
    "        0.9089, 0.9321, 0.9576, 1.    ]),\n",
    " 'prec': array([0.0542, 0.0865, 0.0956, 0.1045, 0.1091, 0.1098, 0.1104, 0.1134,\n",
    "        0.1134, 0.1135, 0.1124, 0.1107, 0.1099, 0.1091, 0.1078, 0.1063,\n",
    "        0.1042, 0.101 , 0.0982, 0.0956]),\n",
    " 'ndcg': array([0.0476, 0.0847, 0.1174, 0.1492, 0.1765, 0.2015, 0.2242, 0.252 ,\n",
    "        0.2704, 0.2906, 0.3056, 0.3199, 0.3327, 0.346 , 0.3559, 0.3675,\n",
    "        0.3762, 0.3822, 0.3878, 0.3972])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8216ea7b7f3c4a9e18e630d57ffc6db6",
     "grade": false,
     "grade_id": "cell-cadab41b9a8671ee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate MF on all users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4a4bd6ed7a25fa676ec2622c9a634b5",
     "grade": false,
     "grade_id": "cell-402b9013346624ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%time metrics = evl.eval_model(mfr)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5afd050fbcff8332dbcad1d31d2fc6a",
     "grade": false,
     "grade_id": "cell-a3d7f732a54a00e1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "evaluated on 665 users\n",
    "\n",
    "CPU times: user 2.78 s, sys: 358 ms, total: 3.14 s\n",
    "Wall time: 3.07 s\n",
    "{'hits': 1.9125874125874125,\n",
    " 'ap': 0.45428173065156513,\n",
    " 'rec': array([0.2604, 0.3981, 0.4548, 0.503 , 0.5359, 0.5507, 0.57  , 0.5879,\n",
    "        0.6033, 0.6223, 0.6469, 0.6607, 0.6769, 0.703 , 0.7293, 0.7503,\n",
    "        0.7777, 0.8235, 0.8962, 1.    ]),\n",
    " 'prec': array([0.4073, 0.3243, 0.2558, 0.2168, 0.1867, 0.1617, 0.1431, 0.1298,\n",
    "        0.1193, 0.1114, 0.106 , 0.1005, 0.0959, 0.093 , 0.0909, 0.0882,\n",
    "        0.0867, 0.0876, 0.0905, 0.0956]),\n",
    " 'ndcg': array([0.3694, 0.4236, 0.4456, 0.4672, 0.4793, 0.4843, 0.4912, 0.4971,\n",
    "        0.5023, 0.5086, 0.5154, 0.5195, 0.5244, 0.5315, 0.5385, 0.5435,\n",
    "        0.5507, 0.5623, 0.5792, 0.6012])}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "beb33984c533da92aa5086035a13f547",
     "grade": false,
     "grade_id": "cell-a905e4e870ada06a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Evaluate CB on all users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6502887df3f8436bd3b3a6e27525398",
     "grade": false,
     "grade_id": "cell-b7ca5c872d5afdec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%time metrics = evl.eval_model(cbr)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42c9f1697d2342f167d09a16db0d9ce9",
     "grade": false,
     "grade_id": "cell-6d55db3a671b16d4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**EXPECTED OUTPUT:**\n",
    "```\n",
    "evaluated on 665 users\n",
    "\n",
    "CPU times: user 1.86 s, sys: 195 ms, total: 2.05 s\n",
    "Wall time: 2.07 s\n",
    "{'hits': 1.9125874125874125,\n",
    " 'ap': 0.25669138784805334,\n",
    " 'rec': array([0.08  , 0.14  , 0.1938, 0.2567, 0.3136, 0.3609, 0.4126, 0.4441,\n",
    "        0.4798, 0.5209, 0.5653, 0.6126, 0.6649, 0.7043, 0.7383, 0.7868,\n",
    "        0.8246, 0.882 , 0.9324, 1.    ]),\n",
    " 'prec': array([0.1591, 0.1364, 0.1265, 0.1233, 0.122 , 0.1174, 0.1146, 0.1088,\n",
    "        0.1043, 0.1021, 0.1001, 0.0982, 0.0982, 0.0962, 0.0943, 0.0944,\n",
    "        0.0937, 0.0941, 0.0945, 0.0956]),\n",
    " 'ndcg': array([0.1302, 0.1502, 0.1763, 0.2044, 0.2289, 0.2472, 0.2665, 0.2778,\n",
    "        0.2901, 0.3032, 0.3155, 0.3287, 0.3431, 0.3535, 0.3632, 0.3766,\n",
    "        0.3868, 0.4017, 0.4135, 0.4292])}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "# feel free to use this field for additional tests"
   },
   "outputs": [],
   "source": [
    "# feel free to use this field for additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "# feel free to use this field for additional tests"
   },
   "outputs": [],
   "source": [
    "# feel free to use this field for additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "# feel free to use this field for additional tests"
   },
   "outputs": [],
   "source": [
    "# feel free to use this field for additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "# feel free to use this field for additional tests"
   },
   "outputs": [],
   "source": [
    "# feel free to use this field for additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": "# feel free to use this field for additional tests"
   },
   "outputs": [],
   "source": [
    "# feel free to use this field for additional tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ebde3599a9a5e27c0ee0e76a6eb463",
     "grade": true,
     "grade_id": "get_ground_truth_default_solution-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06a20310b7e38af274f69db33fc8c165",
     "grade": true,
     "grade_id": "get_ranking_metrics-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "revert": ""
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
